[2023-08-14 16:40:56,786][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2023-08-14 16:40:56,786][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.







Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:27<00:00,  3.91s/it]
trainable params: 1949696 || all params: 6245533696 || trainable%: 0.031217444255383614
[2023-08-14 16:41:44,293][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank: 0
[2023-08-14 16:41:44,294][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.
Installed CUDA version 11.2 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /home/zhangyaqing/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/zhangyaqing/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Installed CUDA version 11.2 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Time to load cpu_adam op: 2.372410297393799 seconds
Parameter Offload: Total persistent parameters: 2312192 in 141 params












   12/17020 [..............................] - ETA: 11:41:27 - loss: 1.6103
[7m   Time consume:                0 hours 1 minutes           
[7m                      GPU Memory Usage                      
[7m       Before training                    0.03gb            
[7m        After training                    9.79gb            
[7m     Peak during training                 9.79gb            
[7m  Total Peak during training              9.83gb            
[7m                                                            
[7m                      CPU Memory Usage                      
[7m       Before training                   17.78gb            
[7m        After training                    0.00gb            
[7m     Peak during training                 0.00gb            
[7m  Total Peak during training             17.78gb            
Error executing job with overrides: []
Traceback (most recent call last):
  File "/work/zhangyaqing/ChatGLM2-6B/accelerate_deepspeed/peft_lora_acc_ds_zero.py", line 211, in main
    outputs = model(**batch)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1769, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/peft/peft_model.py", line 678, in forward
    return self.base_model(
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/zhangyaqing/.cache/huggingface/modules/transformers_modules/chatglm2-6b/modeling_chatglm.py", line 694, in forward
    transformer_outputs = self.transformer(input_ids=input_ids, position_ids=position_ids, attention_mask=attention_mask,
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/zhangyaqing/.cache/huggingface/modules/transformers_modules/chatglm2-6b/modeling_chatglm.py", line 626, in forward
    hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(inputs_embeds, full_attention_mask,
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/zhangyaqing/.cache/huggingface/modules/transformers_modules/chatglm2-6b/modeling_chatglm.py", line 496, in forward
    hidden_states, kv_cache = layer(hidden_states, attention_mask, rotary_pos_emb, kv_cache=kv_caches[index], use_cache=use_cache)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/zhangyaqing/.cache/huggingface/modules/transformers_modules/chatglm2-6b/modeling_chatglm.py", line 428, in forward
    attention_output, kv_cache = self.self_attention(layernorm_output, attention_mask, rotary_pos_emb, kv_cache=kv_cache, use_cache=use_cache)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/zhangyaqing/.cache/huggingface/modules/transformers_modules/chatglm2-6b/modeling_chatglm.py", line 349, in forward
    output = self.dense(context_layer)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py", line 106, in zero3_linear_wrap
    return LinearFunctionForZeroStage3.apply(input, weight)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py", line 98, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/home/zhangyaqing/zhangyaqing/miniconda3/envs/ChatGLM2-6B/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py", line 57, in forward
    output = input.matmul(weight.t())
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 9.79 GiB already allocated; 10.50 MiB free; 10.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.